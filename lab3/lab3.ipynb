{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_path = \"C:\\\\Programming\\\\univer\\\\NPL\\\\Lab2\\\\aclImdb\"\n",
    "\n",
    "pos_train_path = arc_path + \"\\\\train\\\\pos\"\n",
    "neg_train_path = arc_path + \"\\\\train\\\\neg\"\n",
    "\n",
    "pos_file_names = os.listdir(pos_train_path)\n",
    "neg_file_names = os.listdir(neg_train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound = 30000\n",
    "\n",
    "error_list = []\n",
    "\n",
    "pos_list = []\n",
    "for index, pos_file_name in enumerate(pos_file_names):\n",
    "    with open(f\"{pos_train_path}\\\\{pos_file_name}\", \"r\") as file:\n",
    "        try:\n",
    "            pos_list.append(file.read())\n",
    "        except:\n",
    "            error_list.append(f\"pos: {pos_file_name}\")\n",
    "    if index + 1 >= bound:\n",
    "        break\n",
    "\n",
    "neg_list = []\n",
    "for index, neg_file_name in enumerate(neg_file_names):\n",
    "    with open(f\"{neg_train_path}\\\\{neg_file_name}\", \"r\") as file:\n",
    "        try:\n",
    "            neg_list.append(file.read())\n",
    "        except:\n",
    "            error_list.append(f\"neg: {neg_file_name}\")\n",
    "    if index + 1 >= bound:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head:\n",
      "                                                 pos  \\\n",
      "0  Bromwell High is a cartoon comedy. It ran at t...   \n",
      "\n",
      "                                                 neg  \n",
      "0  Story of a man who has unnatural feelings for ...  \n",
      "\n",
      "Tail\n",
      "                                                     pos  \\\n",
      "12496  Working-class romantic drama from director Mar...   \n",
      "\n",
      "                                                     neg  \n",
      "12496  This is one of the dumbest films, I've ever se...  \n"
     ]
    }
   ],
   "source": [
    "min_len = min(len(pos_list), len(neg_list))\n",
    "df = pd.DataFrame.from_dict({\"pos\" : pos_list[:min_len], \"neg\": neg_list[:min_len]})\n",
    "\n",
    "print(\"Head:\")\n",
    "print(df.head(1))\n",
    "\n",
    "print(\"\\nTail\")\n",
    "print(df.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_docs = df['pos'].tolist() \n",
    "neg_docs = df['neg'].tolist()\n",
    "\n",
    "reviews = pos_docs + neg_docs\n",
    "labels = [1] * len(pos_docs) + [0] * len(neg_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000 \n",
    "maxlen = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# print(X_train_sequences[100])\n",
    "# print(type(tokenizer.index_word))\n",
    "# print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=maxlen)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=maxlen)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=128))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 32ms/step - accuracy: 0.7205 - loss: 0.5162 - val_accuracy: 0.8520 - val_loss: 0.3372\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 31ms/step - accuracy: 0.9069 - loss: 0.2393 - val_accuracy: 0.8508 - val_loss: 0.3576\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 31ms/step - accuracy: 0.9437 - loss: 0.1534 - val_accuracy: 0.8412 - val_loss: 0.4583\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 30ms/step - accuracy: 0.9725 - loss: 0.0839 - val_accuracy: 0.8324 - val_loss: 0.5858\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 30ms/step - accuracy: 0.9840 - loss: 0.0516 - val_accuracy: 0.8328 - val_loss: 0.6301\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 30ms/step - accuracy: 0.9898 - loss: 0.0312 - val_accuracy: 0.8338 - val_loss: 0.7526\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 30ms/step - accuracy: 0.9935 - loss: 0.0217 - val_accuracy: 0.8014 - val_loss: 0.6933\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 30ms/step - accuracy: 0.9844 - loss: 0.0422 - val_accuracy: 0.8292 - val_loss: 0.8281\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.9960 - loss: 0.0137 - val_accuracy: 0.8334 - val_loss: 0.9637\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.9968 - loss: 0.0112 - val_accuracy: 0.8308 - val_loss: 1.0026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22dd4c8a750>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "model.fit(X_train_padded, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_padded, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step\n",
      "pos: This movie was absolutely fantastic and kept me on the edge of my seat!\n",
      "neg: I found the plot to be boring and predictable.\n",
      "pos: The acting was superb, and the storyline was captivating.\n",
      "neg: It was a total waste of time; I couldn't wait for it to end.\n",
      "pos: An emotional rollercoaster that left me speechless.\n",
      "pos: A brilliant masterpiece that I would recommend to everyone.\n",
      "pos: I loved the character development throughout the movie.\n",
      "neg: Unfortunately, it didn't live up to the hype; very disappointing.\n",
      "neg: That was the worst film I've ever seen\n",
      "pos: I don't think that was good choise to split this film in two parts, but in general I liked it\n"
     ]
    }
   ],
   "source": [
    "new_reviews = [\n",
    "    \"This movie was absolutely fantastic and kept me on the edge of my seat!\",\n",
    "    \"I found the plot to be boring and predictable.\",\n",
    "    \"The acting was superb, and the storyline was captivating.\",\n",
    "    \"It was a total waste of time; I couldn't wait for it to end.\",\n",
    "    \"An emotional rollercoaster that left me speechless.\",\n",
    "    \"A brilliant masterpiece that I would recommend to everyone.\",\n",
    "    \"I loved the character development throughout the movie.\",\n",
    "    \"Unfortunately, it didn't live up to the hype; very disappointing.\",\n",
    "    \"That was the worst film I've ever seen\",\n",
    "    \"I don't think that was good choise to split this film in two parts, but in general I liked it\"\n",
    "]\n",
    "\n",
    "new_reviews_sequences = tokenizer.texts_to_sequences(new_reviews)\n",
    "new_reviews_padded = pad_sequences(new_reviews_sequences, maxlen=maxlen)\n",
    "\n",
    "predictions = model.predict(new_reviews_padded)\n",
    "\n",
    "for i, prediction in enumerate(predictions):\n",
    "    predicted_class = (prediction[0] > 0.5).astype(int)\n",
    "    if predicted_class == 1:\n",
    "        print(f\"pos: {new_reviews[i]}\")\n",
    "    else:\n",
    "        print(f\"neg: {new_reviews[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
